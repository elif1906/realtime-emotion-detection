{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBERTA_MLM_Esp.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3k3pb_8BN70h"},"source":["## **A modification of the HuggingFace tutorial for training a transformer-encoder model (RoBERTa) on a new language, specifically Esperanto**"]},{"cell_type":"code","metadata":{"id":"Lz-PbTa9N2xY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXKWVBQpQMYc"},"source":["# Change the present working directory to the parent directory for WikiText\n","import os\n","os.chdir('/content/drive/MyDrive/Data/')\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11C-iG_NQpYC"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aiG-3y7mIu0G"},"source":["!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i44nmARbIO0O"},"source":["### **Section 1: Generate byte-level BPE tokens for a corpus**\n","**Section 1.1: Train the tokenizer**"]},{"cell_type":"code","metadata":{"id":"Nm99vBEBM5Kv"},"source":["# Download the Esperanto data set\n","!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1A7jVYz5RCNl"},"source":["from tokenizers import Tokenizer\n","from tokenizers.models import BPE\n","\n","tokenizer = Tokenizer(BPE(unk_token=\"<unk>\")) #BytePair Encoding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aD1h2HK7RP7T"},"source":["import tokenizers\n","\n","tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.ByteLevel() # Byte-level BPE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-AmRxQ7rbGE"},"source":["tokenizer.enable_truncation(512)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SefM_Lr7RNwt"},"source":["from tokenizers.trainers import BpeTrainer\n","trainer = BpeTrainer(special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aF5f1wYRYaz"},"source":["# Assume that \"oscar.eo.txt\" data set is present in the directory \"Esperanto_Input\"\n","files = [\"./Esperanto_Input/oscar.eo.txt\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMFZM2ScRkPz"},"source":["tokenizer.train(files, trainer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdtsdeTNCApn"},"source":["# Use of decoder ensures that the trained model generates the output in the original language \n","# Else the garbled output seen by opening \"tokenizer.json\" file in a text editor is seen in the front-end as well\n","from tokenizers import decoders\n","tokenizer.decoder = decoders.ByteLevel()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wkj2ajN8RkU4"},"source":["tokenizer.save(\"./Esperanto_Output/tokenizer.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUUrRW1CRmnW"},"source":["**Section 1.2: Load and test the tokenizer**\n"]},{"cell_type":"code","metadata":{"id":"vzyQBE7QSOJr"},"source":["tokenizer_load = Tokenizer.from_file(\"./Esperanto_Output/tokenizer.json\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"--gn7AEqSxLZ"},"source":["# Use of RoBERTa's special tokens at beginning and end of sentence\n","tokenizer_load.post_processor = tokenizers.processors.RobertaProcessing(sep=(\"</s>\", tokenizer_load.token_to_id(\"</s>\"))\n","                                                                  , cls=(\"<s>\", tokenizer_load.token_to_id(\"<s>\")))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gxe4zPcdUVz7"},"source":["output = tokenizer_load.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n","print(output.tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DdFVVF_oUeYy"},"source":["print(output.type_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbQYha1qByYe"},"source":["# Perform this step to generate non-garbled characters\n","# from tokenizers import decoders\n","# tokenizer_load.decoder = decoders.ByteLevel()\n","tokenizer_load.decode(output.ids)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L75QE_2IIZc_"},"source":["### **Section 2: Train a masked LM using the tokenizer trained & saved in Section 1**"]},{"cell_type":"code","metadata":{"id":"Pksgg-naKBi_"},"source":["from transformers import RobertaConfig\n","\n","config = RobertaConfig(\n","    vocab_size=30000,  # value of 30K was chosen as the tokenizer was trained with a default value of 30K\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=1,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fCkD9oPjQsGG"},"source":["# Save config.json\n","config.to_json_file('./Esperanto_Output/config.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndXgvTwOKSAQ"},"source":["from transformers import RobertaTokenizerFast\n","\n","# class transformers.RobertaTokenizerFast(vocab_file=None, merges_file=None, tokenizer_file=None, errors='replace'\n","#                                       , bos_token='<s>', eos_token='</s>', sep_token='</s>', cls_token='<s>', unk_token='<unk>', pad_token='<pad>'\n","#                                       , mask_token='<mask>', add_prefix_space=False, **kwargs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jub0bwP0KSHQ"},"source":["tokenizer_new = RobertaTokenizerFast.from_pretrained(\"./Esperanto_Output\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cJGCrSJMT-r"},"source":["from transformers import RobertaForMaskedLM\n","model = RobertaForMaskedLM(config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mmVrrqFMY96"},"source":["from transformers import DataCollatorForLanguageModeling\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer_new, mlm=True, mlm_probability=0.15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"39yLkrFGUw_e"},"source":["# Split the file into multple sub-files so that the model can be trained in less than 1 hr using a free Google Colab account\n","!mkdir ./shards\n","!split -a 40 -l 25600 -d \"./Esperanto_Input/oscar.eo.txt\" ./shards/shard_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VZs5-eZZUxlw"},"source":["import glob\n","files = glob.glob('./shards/*')\n","# files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Col73VQOVS18"},"source":["from datasets import load_dataset\n","# dataset = load_dataset('text', data_files=files, split='train')\n","dataset = load_dataset('text', data_files=files[0], split='train') #Use only one batch of 256000 examples"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEIvajHgUxrp"},"source":["def encode(examples):\n","  return tokenizer_new(examples['text'], truncation=True, padding='max_length', max_length=512)\n","\n","dataset = dataset.map(encode, batched=True) # Apply the \"encode\" function to all elements of \"dataset\" which is passed as \"example\" variable\n","dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F5BzC3FHMeln"},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./Esperanto_Output\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_device_train_batch_size=8, # lowered the batch size from 64\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_R6czungT4Pf"},"source":["%%time\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o5ZAzvViFWNq"},"source":["# EXAMPLE OUTPUT for the above cell\n","\n","# The following columns in the training set  don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: text.\n","# ***** Running training *****\n","#   Num examples = 25603\n","#   Num Epochs = 1\n","#   Instantaneous batch size per device = 8\n","#   Total train batch size (w. parallel, distributed & accumulation) = 8\n","#   Gradient Accumulation steps = 1\n","#   Total optimization steps = 3201\n","\n","# [1425/3201 24:12 < 30:12, 0.98 it/s, Epoch 0.44/1]\n","# Step \tTraining Loss\n","# 500 \t7.665600\n","# 1000 \t7.513300\n","\n","# [3201/3201 54:26, Epoch 1/1]\n","# Step \tTraining Loss\n","# 500 \t7.665600\n","# 1000 \t7.513300\n","# 1500 \t7.345800\n","# 2000 \t7.225700\n","# 2500 \t7.166000\n","# 3000 \t7.074200\n","\n","# Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","# CPU times: user 54min 2s, sys: 11.4 s, total: 54min 14s\n","# Wall time: 54min 27s\n","\n","# TrainOutput(global_step=3201, training_loss=7.3185122203022495, metrics={'train_runtime': 3267.2692, 'train_samples_per_second': 7.836, 'train_steps_per_second': 0.98, 'total_flos': 3393922222964736.0, 'train_loss': 7.3185122203022495, 'epoch': 1.0})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pzz2TCqfTuhS"},"source":["trainer.save_model(\"./Esperanto_Model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-uPSV0ziLLqe"},"source":["### **Section 3: Load and test the trained masked LM**"]},{"cell_type":"code","metadata":{"id":"yP2tBjjPHZG1"},"source":["from transformers import pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uR1RSrYLaE5"},"source":["# fill_mask = pipeline(\"fill-mask\", model=\"./Esperanto_Model\", tokenizer=\"./Esperanto_Output\")\n","fill_mask = pipeline(\"fill-mask\", model=\"./Esperanto_Model\", tokenizer=\"./Esperanto_Model\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HknWgXoTLjff"},"source":["fill_mask(\"La suna <mask>\")\n","\n","# [{'score': 0.013835701160132885,\n","#   'sequence': 'La ƒ†suna ƒ†,',\n","#   'token': 16,\n","#   'token_str': ','},\n","#  {'score': 0.01227512676268816,\n","#   'sequence': 'La ƒ†suna ƒ† -',\n","#   'token': 17,\n","#   'token_str': '-'},\n","#  {'score': 0.009938908740878105,\n","#   'sequence': 'La ƒ†suna ƒ† :',\n","#   'token': 30,\n","#   'token_str': ':'},\n","#  {'score': 0.008791058324277401,\n","#   'sequence': 'La ƒ†suna ƒ† ƒ†la',\n","#   'token': 228,\n","#   'token_str': 'ƒ†la'},\n","#  {'score': 0.008423665538430214,\n","#   'sequence': 'La ƒ†suna ƒ† ƒ†kaj',\n","#   'token': 252,\n","#   'token_str': 'ƒ†kaj'}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"74jb8UJXL3_B"},"source":["fill_mask(\"Mi estas <mask>\")"],"execution_count":null,"outputs":[]}]}